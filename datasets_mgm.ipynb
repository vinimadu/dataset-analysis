{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2255c1a",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11588b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDD100k', 'BelgiumTS', 'CCTSDB', 'GTSDB', 'TT100k', 'videos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload all modules\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import fiftyone as fo\n",
    "# session = fo.launch_app(auto=False)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "import utils\n",
    "import json\n",
    "\n",
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4df9b0",
   "metadata": {},
   "source": [
    "# Dataset Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"GTSDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.resize_samples(dataset, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.delete_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507447ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_datasets(['GTSDB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c47c25",
   "metadata": {},
   "source": [
    "# Mapillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load mapillary dataset\n",
    "\n",
    "name = \"Mapillary\"\n",
    "\n",
    "if not fo.dataset_exists(name):\n",
    "\n",
    "    dataset_dir = \"../datasets/mapillary/images\"\n",
    "\n",
    "    dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_type=fo.types.ImageDirectory,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    dataset.persistent = True\n",
    "    dataset.compute_metadata()\n",
    "\n",
    "    annotations_dir = \"../datasets/mapillary/mtsd_v2_fully_annotated/annotations\"\n",
    "\n",
    "    dataset.tag_samples(name)\n",
    "\n",
    "    with fo.ProgressBar() as pb:\n",
    "        for sample in pb(dataset):\n",
    "                json_file = sample.filename.replace('.jpg','.json').replace('.png','.json')\n",
    "                json_file = os.path.join(annotations_dir, json_file)\n",
    "                if not os.path.exists(json_file):\n",
    "                    continue\n",
    "                    #dataset.delete_samples([sample])\n",
    "                else:\n",
    "                    with open(json_file) as f:\n",
    "                        data = json.load(f)\n",
    "                    detections = []\n",
    "\n",
    "                    for object in data['objects']:\n",
    "                        # Convert to [top-left-x, top-left-y, width, height]\n",
    "                        # in relative coordinates in [0, 1] x [0, 1]\n",
    "                        W, H = sample.metadata.width, sample.metadata.height\n",
    "                        x1, x2, y1, y2 = object['bbox']['xmin']/W, object['bbox']['xmax']/W, object['bbox']['ymin']/H, object['bbox']['ymax']/H\n",
    "                        rel_box = [x1, y1, (x2 - x1), (y2 - y1)]\n",
    "                        detections.append(\n",
    "                                        fo.Detection(\n",
    "                                            label=object['label'],\n",
    "                                            bounding_box=rel_box,   \n",
    "                                        )\n",
    "                                    )\n",
    "                    if len(detections) != 0:                   \n",
    "                        sample['detections'] = fo.Detections(detections=detections)\n",
    "                    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82084d",
   "metadata": {},
   "source": [
    "# Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fo.dataset_exists('master_dataset'):\n",
    "    master_dataset = fo.Dataset('master_dataset')\n",
    "    master_dataset.persistent = True\n",
    "\n",
    "    for dataset_name in fo.list_datasets():\n",
    "        master_dataset.merge_samples(fo.load_dataset(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #joining all datasets\n",
    "# if not fo.dataset_exists('master_dataset'):\n",
    "#     master_dataset = fo.Dataset('master_dataset')\n",
    "\n",
    "#     for dataset_name in fo.list_datasets():\n",
    "#         dataset = fo.load_dataset(dataset_name)\n",
    "#         if dataset_name == 'master_dataset' or dataset_name == 'my-videos':\n",
    "#             continue\n",
    "#         for sample in dataset:\n",
    "#             if len(master_dataset.match(F('filepath')==sample.filepath)) == 0:\n",
    "#                 master_dataset.add_sample(sample)\n",
    "\n",
    "#     master_dataset.persistent = True\n",
    "#     master_dataset.save()\n",
    "\n",
    "# else:\n",
    "#     master_dataset = fo.load_dataset('master_dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd81eae",
   "metadata": {},
   "source": [
    "# BRTS Pierre2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9908201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1364/1364 [128.0ms elapsed, 0s remaining, 10.8K samples/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1363/1363 [00:08<00:00, 161.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from fiftyone import ViewField as F\n",
    "from PIL import Image\n",
    "images_dir = '/home/madu/GIT/FOdatasets/BRTS/Brazilian Vertical Traffic Signs and Lights Dataset/images'\n",
    "annotations_dir = images_dir.replace('images','annotations')\n",
    "name = 'BRTS'\n",
    "\n",
    "label_map = {\n",
    "    '000000': 'pare',\n",
    "    '000001': 'preferencia',\n",
    "    '000003': 'proibido_virar_esquerda',\n",
    "    '000004': 'proibido_virar_direira',\n",
    "    '000007': 'proibido_estacionar',\n",
    "    '000008': 'permitido_estacionar',\n",
    "    '000009': 'proibido_parar_estacionar',\n",
    "    '000023': 'velocidade_80',\n",
    "    '000025': 'lombada',\n",
    "    '000028': 'sentido_obrigatorio',\n",
    "    '000035': 'trafego_direita',\n",
    "    '000040': 'exclusivo_onibus',\n",
    "    '000042': 'exclusivo_ciclistas',\n",
    "    '000051': 'amarelo',\n",
    "    '000052': 'vermelho',\n",
    "    '000053': 'verde',\n",
    "\n",
    "}\n",
    "\n",
    "# Function to parse XML and extract data\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    W = int(root.find('size/width').text)\n",
    "    H = int(root.find('size/height').text)\n",
    "\n",
    "    detections = []\n",
    "    for obj in root.findall('object'):\n",
    "        label = label_map[obj.find('name').text]\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(float(bbox.find('xmin').text)) / W\n",
    "        ymin = int(float(bbox.find('ymin').text)) / H\n",
    "        xmax = int(float(bbox.find('xmax').text)) / W\n",
    "        ymax = int(float(bbox.find('ymax').text)) / H\n",
    "        x0 = min(xmin, xmax)\n",
    "        y0 = min(ymin, ymax)\n",
    "        w = abs(xmax - xmin)\n",
    "        h = abs(ymax - ymin)\n",
    "\n",
    "        rel_box = [x0, y0, w, h]\n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=rel_box,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return detections\n",
    "\n",
    "for img in os.listdir(images_dir):\n",
    "    if '@' in img:\n",
    "        os.remove(os.path.join(images_dir,img))\n",
    "\n",
    "for annot in os.listdir(annotations_dir):\n",
    "    if annot.endswith('.xml') and '@' in annot:\n",
    "        os.remove(os.path.join(annotations_dir,annot))\n",
    "\n",
    "if not fo.dataset_exists(name):\n",
    "    dataset = fo.Dataset.from_dir(\n",
    "            dataset_dir=images_dir,\n",
    "            dataset_type=fo.types.ImageDirectory,\n",
    "            name=name,\n",
    "        )\n",
    "    \n",
    "    for xml_file in tqdm(os.listdir(annotations_dir)):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            xml_file_path = os.path.join(annotations_dir, xml_file)\n",
    "            filename = xml_file.replace('xml','jpg')\n",
    "            detections = parse_xml(xml_file_path)\n",
    "            view = dataset.match(F(\"filepath\").contains_str(filename))\n",
    "            if len(view) > 0:\n",
    "                sample = view.first()\n",
    "                if len(detections) != 0:  \n",
    "                    sample['detections'] = fo.Detections(detections=detections)\n",
    "                sample.save()\n",
    "    dataset.persistent = True\n",
    "    #dataset.compute_metadata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637339a",
   "metadata": {},
   "source": [
    "# Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.Dataset.from_dir(\n",
    "        dataset_dir='/home/madu/GIT/FOdatasets/videos',\n",
    "        dataset_type=fo.types.VideoDirectory,\n",
    "        name='videos',\n",
    "    )\n",
    "dataset.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee79d1e",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91885340",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset('GTSDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb75e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.untag_samples(['train','val'])\n",
    "dataset.tag_samples('train')\n",
    "val = dataset.take(int(len(dataset)*0.2))\n",
    "val.tag_samples('val')\n",
    "val.untag_samples('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f689a7",
   "metadata": {},
   "source": [
    "# Export dataset\n",
    "## Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# The splits to export\n",
    "splits = [\"train\", \"val\"]\n",
    "\n",
    "one_class = False\n",
    "\n",
    "# All splits must use the same classes list\n",
    "\n",
    "export_dir = f'../datasets/{dataset.name}'\n",
    "\n",
    "if one_class:\n",
    "    export_dir += '-1class'\n",
    "    map_dict = {k: 'sign' for v, k in enumerate(dataset.distinct('detections.detections.label'))}\n",
    "    view = dataset.map_labels(\n",
    "        'detections',\n",
    "        map=map_dict,\n",
    "    )\n",
    "else:\n",
    "    view = dataset\n",
    "\n",
    "classes = view.distinct('detections.detections.label')\n",
    "classes.sort()\n",
    "\n",
    "# Export the splits\n",
    "for split in splits:\n",
    "    split_view = view.match_tags(split)\n",
    "    split_view.export(\n",
    "        export_dir=export_dir,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field='detections',\n",
    "        split=split,\n",
    "        classes=classes,\n",
    "    )\n",
    "\n",
    "with open(f'{export_dir}/dataset.yaml', 'r') as f:\n",
    "    dataset_yaml = yaml.safe_load(f)\n",
    "\n",
    "dataset_yaml['path'] = f'./datasets/{dataset.name}'\n",
    "with open(f'{export_dir}/dataset.yaml', 'w') as f:\n",
    "    yaml.dump(dataset_yaml, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d404b",
   "metadata": {},
   "source": [
    "## TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c98f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "export_dir = f'../datasets/{dataset.name}_TFRecord'\n",
    "label_field = \"detections\"  # for example\n",
    "\n",
    "# Export the dataset\n",
    "dataset.take(200).export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.TFObjectDetectionDataset,\n",
    "    label_field=label_field,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hailo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
